---
title: "Práctica 1"
author: "Abraham Ramírez"
date: "23 November 2017"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv('data.csv')
library(ggplot2)
```

#Ejercicio 1

Test the TestGradientDescent function with the training set (4_1_data.csv). Obtain the confusion matrix.

Creamos una funcion que nos regresa una lista con dos data frames, uno de test y otro de train

```{r}
muestra <- function(datos,porcentaje,semilla){
  set.seed(semilla)
  train <- sample(nrow(datos), porcentaje*nrow(datos),replace = F)
  return(list(datos[train,],datos[-train,]))
}
```

Usamos la función y creamos nuestras variables de train y test

```{r}
datos <- muestra(data,.7,1234)
train <- datos[[1]]
test <- datos[[2]]
```

Agregamos una columna de 1 y convertimos los df a matrices.

```{r}
x.train <- as.matrix(cbind(rep(1,nrow(train[,1:2])),train[,1:2]))
y.train <- as.matrix(train[,3])
```

Usaremos la función sigmoide vista en clase.

```{r}
Sigmoid <- function(x){
  1/(1+exp(-x))
}
```

Usaremos la función de coste vista en clase.

```{r}
CostFunction <- function(parameters,X,Y){
  n <- nrow(X)
  g <- Sigmoid(X %*% parameters)
  J <- (1/n) * sum((-Y * log(g)) - ((1-Y) * log(1-g)))
  return(J)
}
```

Usaremos la función vista en clase para calcular el descenso del gradiente.

```{r}
TestGradientDescent <- function(iterations = 1200, X, Y) {
  
  parameters <- rep(0, ncol(X))

  print(paste("Initial Cost Function value: ", 
              convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
  
  parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y, 
                                   control = list(maxit = iterations))

  parameters <- parameters_optimization$par
  

  print(paste("Final Cost Function value: ", 
              convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
  
  return(parameters) 
}
```

Obtenemos los parametros adecuados para hacer la clasificación.

```{r}
parametros <- TestGradientDescent(X = x.train, Y = y.train)
parametros
```

Obtenemos la predicción sobre la matriz de test

```{r}
test <- as.matrix(cbind(rep(1,nrow(test)),test))
prediccion <-ifelse(abs(1-Sigmoid(test[,1:3]%*%parametros))>abs(0-Sigmoid(test[,1:3]%*%parametros)),0,1)
test<-as.data.frame(cbind(test,prediccion))
```

Obtenemos la matriz de confusión

```{r}
table(test$label,test$V5,dnn = c('Original','Predicción'))
```

##¿Es estable el modelo?

```{r}
datos2 <- muestra(data,.7,1)
train2 <- datos2[[1]]
test2 <- datos2[[2]]
x.train2 <- as.matrix(cbind(rep(1,nrow(train2[,1:2])),train2[,1:2]))
y.train2 <- as.matrix(train2[,3])
parametros2 <- TestGradientDescent(X = x.train2, Y = y.train2)
parametros2
test2 <- as.matrix(cbind(rep(1,nrow(test2)),test2))
prediccion2 <-ifelse(abs(1-Sigmoid(test2[,1:3]%*%parametros2))>abs(0-Sigmoid(test2[,1:3]%*%parametros2)),0,1)

prediccion3 <-ifelse(abs(1-Sigmoid(test2[,1:3]%*%parametros))>abs(0-Sigmoid(test2[,1:3]%*%parametros)),0,1)

test2<-as.data.frame(cbind(test2,prediccion2))
test2<-as.data.frame(cbind(test2,prediccion3))
table(test2$prediccion3,test2$V5,dnn = c('Original','Nuevo Modelo'))
```

Podemos concluir que el modelo es estable ya que cambiando el conjunto de train y test nuestras predicciones sobre los individuos muy parecidas. 
Como podemos ver en la tabla sólo el 3% de los individuos son clasificados de diferente manera en el modelo original y en el nuevo.

#Ejercicio 2

Obtain a graph representing how the cost function evolves depending of the number of iterations.

Modificamos un poco la función de descenso de gradiente vista en clase para poder guardar los errores cometidos por iteración.
```{r}
TestGradientDescent2 <- function(iterations = 1200, X, Y) {
  
  parameters <- rep(0, ncol(X))
  
  parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y, 
                                   control = list(maxit = iterations))
  parameters <- parameters_optimization$par
  convergence <<- c(CostFunction(parameters, X, Y))
  
  return(parameters) 
}
```

Creamos una función que nos devuelve un data frame con los errores cometidos por iteración usando la funcion anterior.

```{r}
error <- function(iteraciones,x,y){
  df.iteraciones <- NULL
  i<-1
  while (i<iteraciones) {
    TestGradientDescent2(iterations = i,X = x, Y = y)
    df.iteraciones<-as.data.frame(rbind(df.iteraciones,convergence))
    i<-i+1
  }
  return(df.iteraciones)
}
```


Usamos la función anterior y graficamos.

```{r,cache=TRUE}
df.error <- error(1000,x.train,y.train)
ggplot(df.error) + geom_point(aes(x=1:999,y=V1)) + xlab('Iteraciones') + 
  ylab('Error')
```

Vemos que el error baja en la iteración 100 y se mantiene por lo que podríamos cortar las iteraciones ahí y minimizar el coste computacional.
